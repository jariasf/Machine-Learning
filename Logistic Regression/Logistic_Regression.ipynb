{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "O8XeP1Hz3FnD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression\n",
        "\n",
        "**Author:** Jhosimar George Arias Figueroa"
      ]
    },
    {
      "metadata": {
        "id": "3El4iFJ03Zqm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introdution\n",
        "\n",
        "This notebook is intended to be a guide for a better understanding of the implementation for Logistic Regression. We will implement it from scratch with numpy and with the sklearn package."
      ]
    },
    {
      "metadata": {
        "id": "FggWhwmj36Vi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ]
    },
    {
      "metadata": {
        "id": "CtPL71ey4rxV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oQ9hvImO39Um",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will load data from sklearn"
      ]
    },
    {
      "metadata": {
        "id": "H86XIudJ39vE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import datasets, linear_model\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]\n",
        "y = (iris.target != 0) * 1\n",
        "#y = y[:,np.newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PH6jugkK4LtP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's plot the train and test generated datasets"
      ]
    },
    {
      "metadata": {
        "id": "0Cbaxn-g4TM1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.scatter(X[:,0], X[:,1])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dzZ0ZsMf5z9Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Implementation"
      ]
    },
    {
      "metadata": {
        "id": "hiSsvyqQ52Ca",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will implement Linear Regression first with **numpy** and then with **sklearn**"
      ]
    },
    {
      "metadata": {
        "id": "WEtLqpU93GS0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LogisticRegression:\n",
        "  \n",
        "  def __init__(self, **params):\n",
        "    \"\"\" Initialize global parameters of the class\n",
        "        \n",
        "    Parameters:\n",
        "    ------------\n",
        "        iterations: int, optional\n",
        "             Number of iterations of gradient descent to be performed\n",
        "             \n",
        "        learning_rate: float, optional     \n",
        "             Learning rate used in gradient descent\n",
        "        \n",
        "        verbose: int, optional\n",
        "             Print information during training at each specified iteration\n",
        "        \n",
        "        fit_intercept: boolean, optional\n",
        "             Whether to add the intercept (bias) or not\n",
        "        \n",
        "    \"\"\" \n",
        "    self.iterations = params.get('iterations', 10)\n",
        "    self.alpha = params.get('learning_rate', 1e-3)\n",
        "    self.print_it = params.get('verbose', 1)\n",
        "    self.fit_intercept = params.get('fit_intercept', True)\n",
        "    \n",
        "    \n",
        "  def add_intercept(self, X):\n",
        "    ones = np.ones((X.shape[0], 1))\n",
        "    return np.hstack( (X, ones) )\n",
        "  \n",
        "  \n",
        "  def sigmoid(self, z):\n",
        "    \"\"\" Compute value of the activation function\n",
        "        \n",
        "    Parameters:\n",
        "    ------------\n",
        "        z : numpy array, shape = [n, m]\n",
        "        \n",
        "    Returns:\n",
        "    ---------\n",
        "        Sigmoid functino applied to the input matrix\n",
        "    \"\"\"\n",
        "    return 1/(1 + np.exp(-z))\n",
        "  \n",
        "  \n",
        "  def cost_function(self, X, W, y):\n",
        "    \"\"\" Compute value of the cost function and its gradient\n",
        "        \n",
        "    Parameters:\n",
        "    ------------\n",
        "        X : numpy array, shape = [n_samples, n_features]\n",
        "        W : numpy array, shape = [n_features, num_classes]\n",
        "        y : numpy array, shape = [n_samples, 1]\n",
        "        \n",
        "    Returns:\n",
        "    ---------\n",
        "        cost: value of the cost function given the true and predicted values\n",
        "        grad: value of the cost function's gradient\n",
        "    \"\"\"\n",
        "    \n",
        "    n = X.shape[0]\n",
        "    logit = X.dot(W)\n",
        "    y_pred = self.sigmoid(logit)\n",
        "    loss = np.mean(-y * np.log(y_pred) - (1 - y) * np.log(1 - y_pred))\n",
        "    grad = X.T.dot(y_pred - y)/n\n",
        "    return loss, grad\n",
        "  \n",
        "  \n",
        "  def gradient_descent(self, X, y):\n",
        "    \"\"\" Compute values of parameters W\n",
        "        \n",
        "    Parameters:\n",
        "    ------------\n",
        "        X : numpy array, shape = [n_samples, n_features]\n",
        "        y : numpy array, shape = [n_samples, 1]\n",
        "        \n",
        "    Returns:\n",
        "    ---------\n",
        "        W: parameters after training process\n",
        "        cost_history: history of cost function\n",
        "        \n",
        "    \"\"\"\n",
        "    n, d = X.shape[0], X.shape[1]\n",
        "    W = np.zeros((d, 1))\n",
        "    cost_history = []\n",
        "    for it in range(self.iterations):\n",
        "      loss, grad = self.cost_function(X, W, y)\n",
        "      W = W - self.alpha * grad\n",
        "      cost_history.append(loss)\n",
        "    return W, cost_history\n",
        "  \n",
        "  \n",
        "  def train(self, X,y):\n",
        "    \"\"\" Compute value of parametes W according to training data set and method\n",
        "        \n",
        "    Parameters:\n",
        "    ------------\n",
        "        X : numpy array, shape = [n_samples, n_features]\n",
        "        y : numpy array, shape = [n_samples, 1]\n",
        "        \n",
        "    Returns:\n",
        "    ---------\n",
        "        self: object\n",
        "        \n",
        "    \"\"\" \n",
        "    \n",
        "    # if only one dimension, reshape data\n",
        "    if len(X.shape) == 1:\n",
        "      X = X.reshape(-1,1)\n",
        "    if len(y.shape) == 1:\n",
        "      y = y.reshape(-1,1)\n",
        "    if self.fit_intercept:\n",
        "      X = self.add_intercept(X)\n",
        "      \n",
        "    self.W, cost_history = self.gradient_descent(X, y)\n",
        "    \n",
        "    for it in range(self.iterations):\n",
        "      if self.print_it != 0 and it % self.print_it == 0:\n",
        "        print( \"it: %d, cost: %.3lf\" % (it, cost_history[it]) )\n",
        "    \n",
        "    \n",
        "  def predict_prob(self,X):\n",
        "    \"\"\" Predict values given new data\n",
        "        \n",
        "    Parameters:\n",
        "    ------------\n",
        "        X : numpy array, shape = [n_samples, n_features]\n",
        "        \n",
        "    Returns:\n",
        "    ---------\n",
        "        Predicted values by using the learng parameters W\n",
        "    \"\"\"    \n",
        "    if self.fit_intercept:\n",
        "      X = self.add_intercept(X)\n",
        "    logit = X.dot(self.W)\n",
        "    return self.sigmoid(logit)\n",
        "  \n",
        "  \n",
        "  def predict(self, X, threshold=0.5):\n",
        "    \"\"\" Classify new data given a threshold\n",
        "        \n",
        "    Parameters:\n",
        "    ------------\n",
        "        X : numpy array, shape = [n_samples, n_features]\n",
        "        threshold: float\n",
        "        \n",
        "    Returns:\n",
        "    ---------\n",
        "        Predicted values thresholded by input value\n",
        "    \"\"\"    \n",
        "    return self.predict_prob(X) >= threshold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iRrLg5XH7W76",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(iterations=30000, learning_rate=1e-1,verbose=100)\n",
        "model.train(X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LbexBlHu7h3N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X, 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l6kVCH1M7nOp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(y_pred == y).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UpalJ0uc7pG8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}